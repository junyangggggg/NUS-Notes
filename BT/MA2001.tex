\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}

\title{MA2001 Important Theorems}
\author{Jun Yang Liew}
\date{October 2022}

\begin{document}

\maketitle

\section*{Matrix}

\begin{enumerate}
\item \(A(BC) = (AB)C\)
\item \(A(B_1 + B_2) = AB_1 + AB_2\)
\item \((C_1 + C_2)A = C_1A + C_2A\)
\item \(c(AB) = (cA)B = A(cB)\)
\item \(A0 = 0\)
\item \(AI = A = IA\)
\item \(A^mA^n = A^{m+n}\)
\item \((AB)^2 = (AB)(AB)\)
\item \((A^T)^T = A\) 
\item \((A+B)^T = A^T + B^T\) 
\item \((cA)^T = cA^T\) 
\item then \((AB)^T = B^TA^T\)
\end{enumerate}

\section*{Invertible Matrix}

\begin{enumerate}
\item \(A\) is invertible \(\Leftrightarrow AB = I \) and \(BA = I\)
\item if \(A\) is invertible and \(AB_1 = AB_2\), then \(B_1 = B_2\), B could be non-square matrix
\item The inverse of a matrix is unique
\item \((cA)^{-1} = \frac{1}{c} A^{-1}\)
\item \((A^T)^{-1} = (A^{-1})^T\)
\item \((A^{-1})^{-1} = A\)
\item \((AB)^{-1} = B^{-1}A^{-1}\)
\item \((A^n)^{-1} = (A^{-1})^n\)
\item If \(A\) is singular, then both \(AB\) and \(BA\) are singular
\end{enumerate}

\section*{Determinant}

\begin{enumerate}
\item If \(A\) is an triangular matrix, then \(det(A) = \) product of diagonal entries
\item \(det(A^T) = det(A)\)
\item \(det(AA^{-1}) = det(A) \times det(A^{-1}) = 1\)
\item a square matrix with two identical rows or columns has \(0\) determinant
\item If \(A \xrightarrow{kR_i} B\), then \(det(B) = k \times det(A)\)
\item If \(A \leftrightarrow B\), then \(det(B) = -det(A)\)
\item If \(A \xrightarrow{R_j + kR_i} B\), then \(det(B) = det(A)\)
\item If \(A\) is a \(n \times n\) matrix, then \(det(cA) = c^n det(A)\)
\item \(det(AB) = det(A) \times det(B)\)
\item \(det(A^{-1}) = \frac{1}{det(A)}\)
\item \(A^{-1} = \frac{1}{det(A)} \times adj(A)\)
\end{enumerate}\

\section*{Linear Span}
\begin{enumerate}
\item Linear span = the set of all linear combinations of \( \{c_1 u_1 + c_2 u_2 + ... + c_k u_k \mid c_1, c_2,..., c_k \in \mathbb{R} \} \)
\item $0 \in span(S)$
\item For any $v_1, v_2, ..., v_r \in span(S)$ and $c_1, c_2, ..., c_r \in \mathbb{R}$, $c_1v_1 + c_2v_2 + ... + c_rv_r \in span(S)$
\item $span(u_1, u_2, ..., u_k) \subset span(v_1, v_2, ..., v_k) \Leftrightarrow u_i$ is a linear combination of $v_1, v_2, ..., v_k$
\end{enumerate}

\section*{Subspace}
\begin{enumerate}
\item If it is a span of something, it is a subspace
\item $span{0}$ is the smallest subspace, a.k.a trivial subspace
\item If it satisfies:
\begin{enumerate}
    \item $0 \in span(S)$
    \item For any $v_1, v_2, ..., v_r \in span(S)$ and $c_1, c_2, ..., c_r \in \mathbb{R}$, $c_1v_1 + c_2v_2 + ... + c_rv_r \in span(S)$
\end{enumerate} then it is a subspace
\item The solution set of a homogenous system of linear equations in $n$ variables is a subspace of $\mathbb{R}^n$, and it is called the solution space
\end{enumerate}

\section*{Linear Independence}
\begin{enumerate}
\item If a set of vectors is linearly dependent, then there exists at least one redundant vector in the set
\item If a set of vectors is linearly independent, then tehre is no redundant vector in the set
\item In $\mathbb{R}^n$, if the set has more vector than $n$, then the set must be linearly dependent
\end{enumerate}

\section*{Bases}
\begin{enumerate}
\item let $V$ be a vector space and $S = \{ u_1, u_2, ..., u_k \}$ a subset of $V$, then $S$ is called a basis of $V$ if 
\begin{enumerate}
    \item $S$ is linearly independent
    \item $S$ spans $V$
\end{enumerate}
\item For any $u, v \in V, u = v \Leftrightarrow (u)_S = (v)_S$
\item For any $v_1, v_2, ..., v_r \in V$ and $c_1, c_2, ..., c_r \in \mathbb{R}$,\\$(c_1v_1 + c_2v_2 + ... + c_rv_r)_S = c_1(v_1)_S + c_2(v_2)_S + ... + c_r(v_r)_S$
\item $ v_1, v_2, ..., v_r $ are linearly independent $\Leftrightarrow
$ $ (v_1)_S, (v_2)_S, ..., (v_r)_S $ are linearly independent vectors in $ \mathbb{R}^k $ 
\item $span\{v_1, v_2, ..., v_r\} = V \Leftrightarrow span\{(v_1)_S, (v_2)_S, ..., (v_r)_S\} = \mathbb{R}^k$
\end{enumerate}

\section*{Dimensions}
\begin{enumerate}
\item Let $V$ be a vector space which has a basis with $k$ vectors,
\begin{enumerate}
    \item Any subset of $V$ with more than $k$ vectors is always linearly dependent
    \item Any subset of $V$ with less than $k$ vectors cannot spans $V$
\end{enumerate} this means that every basis for $V$ have the same size $k$
\item The dimension of a vector space $V$, denoted by $dim(V)$, is defined to be the number of vectors in a basis for $V$
\end{enumerate}

\section*{Transition Matrix}
\begin{enumerate}
\item a transition matrix is invertible
\item $P^{-1}$ is the transition matrix with an opposite direction
\item A transition matrix from $S$ to $T$, write it in $S$ is a linear combination of $T$, then form the matrix with \\
$$\begin{pmatrix} [s_1]_T & [s_2]_T & [s_3]_T \end{pmatrix}$$.
\end{enumerate}

\section*{Row Space and Column Space}
\begin{enumerate}
\item The nonzero rows of REF of A form a basis for the row space of REF of A and A
\item Let $W = span\{u_1, u_2, u_3\}$, methods to find the basis of W:
\begin{enumerate}
    \item \begin{enumerate}
        \item Place the vectors as row vectors to form a matrix
        \item Get the row space from REF
        \item The basis of row space is the basis of $W$
    \end{enumerate}
    \item \begin{enumerate}
        \item Place the vectors as column vectors to form a matrix
        \item The basis of the column space is the basis of $W$
        \end{enumerate}
\end{enumerate}
\item A linear system $Ax = b$ is consistent $\Leftrightarrow$ $b$ lies in the column space of $A$
\end{enumerate}

\section*{Ranks}
\begin{enumerate}
\item The row space and the column space of a matrix has the same dimension
\item The rank of a matrix is the dimension of its row space and its column space
\item $rank(0) = 0$ and $rank(I_n) = n$
\item $rank(A) \leq min\{m, n\}$
\item If $rank(A) = min\{m, n\}$, then $A$ is said to have full rank
\item A square matrix $A$ is of full rank $\Leftrightarrow$ $det(A) \neq 0$
\item $rank(A) = rank(A^T)$
\item A linear system $Ax = b$ is consistent $\Leftrightarrow$ $A$ and $(A|b)$ have the same rank
\item $rank(AB) \leq min\{rank(A), rank(B)\}$
\end{enumerate}

\section*{Nullspaces and Nullities}
\begin{enumerate}
\item The solution space of $Ax = 0$ is the nullspace of $A$
\item The dimension of the nullspace of A is the nullity of $A$
\item $rank(A) + nullity(A) =$ the number of columns of $A$
\end{enumerate}

\section*{The Dot Product}
\begin{enumerate}
\item The norm(length) of $u = ||u|| = \sqrt{u_1^2 + u_2^2 + ... + u_k^2}$
\item $d(u, v) = ||u - v|| = \sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + ... + (u_k - v_k)^2}$
\item $u \cdot v = u_1v_1 + u_2v_2 + ... + u_kv_k$
\item $\theta = cos^{-1}(\frac{u \cdot v}{||u|| ||v||})$
\item $u \cdot v = v \cdot u$
\item $(u + v) \cdot w = u \cdot w + v \cdot w$ and $w \cdot (u + v) = w \cdot u + w \cdot v$
\item $(cu) \cdot v = u \cdot (cv) = c(u \cdot v)$
\item $||cu|| = |c| ||u||$
\item $u \cdot u \geq 0; u \cdot u = 0 \Leftrightarrow u = 0$
\end{enumerate}

\section*{Orthogonal and Orthonormal Bases}
\begin{enumerate}
    \item $u$ and $v$ are orthogonal if $u \cdot v = 0$
    \item $\frac{1}{||u||} U$ is a unit vector
    \item An orthogonal set is linearly independent
    \item To show $S$ is an orthogonal basis for $V$, we need to check: \begin{enumerate}
        \item $S$ is orthogonal
        \item $|S| = dim(V)$ or $span(S) = V$
    \end{enumerate}
    \item Let $S = \{u_1, u_2, ..., u_k$ be an orthogonal basis for $V$, then for any $w \in V$: $w = \frac{w\cdot u_1}{u_1 \cdot u_1}u_1 + \frac{w\cdot u_2}{u_2 \cdot u_2}u_2 + ... + \frac{w\cdot u_k}{u_k \cdot u_k}u_k$
    \item From above, $w$ is the projection onto $V$
    \item Gram-Schmidt Process: \\ 
    Let $\{u_1, u_2, ..., u_k\}$ be a basis for $V$ \\
    $v_1 = u_1$ \\
    $v_2 = u_2 - \frac{u_2 \cdot v_1}{v_1 \cdot v_1}v_1$ \\
    $v_3 = u_3 - \frac{u_3 \cdot v_1}{v_1 \cdot v_1}v_1 - \frac{u_3 \cdot v_2}{v_2 \cdot v_2}v_2$ \\   
\end{enumerate}

\section*{Best Approximation}
\begin{enumerate}
    \item Let $Ax = b$ be a linear system, \\
    then $u$ is a least square solution to the system $Ax = b \Leftrightarrow u$ is a solution to $A^TAx = A^Tb$
\end{enumerate}

\section*{Orthogonal Matrices}
\begin{enumerate}
    \item A square matrix $A$ is orthogonal if $A^{-1} = A^T$
    \item $A$ is orthogonal \\
    $\Leftrightarrow$ The rows of $A$ form an orthonormal basis for $\mathbb{R}^n$ \\
    $\Leftrightarrow$ The columns of $A$ form an orthonormal basis for $\mathbb{R}^n$
    
\end{enumerate}

\section*{Eigenvalues and Eigenvectors}
\begin{enumerate}
    \item $\lambda$ is an eigenvalue of $A$ \\
    $\Leftrightarrow Au = \lambda u$ \\
    $\Leftrightarrow \lambda u - Au = 0$ \\
    $\Leftrightarrow (\lambda I - A)u = 0$ \\
    $\Leftrightarrow$ the linear system $(\lambda I - A)u = 0$ has non-trivial solutions
    $\Leftrightarrow det(\lambda I - A) = 0$
    \item Eigenspaces is the solution space of the linear system $(\lambda I - A) = 0$
\end{enumerate}

\section*{Diagonalization}
\begin{enumerate}
    \item A square matrix $A$ is diagonalizable if there exists an invertible matrix $P$ such that $P^{-1}AP$ is a diagonal matrix. $P$ is said to diagonalize $A$
    \item $A$ is diagonalizable $\Leftrightarrow A$ has $n$ linearly independent eigenvectors
    \item Steps to diagonalize a matrix of $n$ order: \begin{enumerate}
        \item Find all distinct eigenvalues $\lambda_1, ..., \lambda_k$
        \item For each eigenvalue find a basis $S_\lambda_i$ for the eigenspace $E_\lambda_i$
        \item Let $S = S_\lambda_1 \cup ... \cup S_lambda_k$ \\
        If $|S| < n$, then $A$ is not diagonalizable \\
        If $|S| = n$, then $A$ is diagonalizable
        \item If an $n$ order matrix $A$ has $n$ distinct eigenvalues, then $A$ is diagonalizable
    \end{enumerate}
\end{enumerate}

section*{Orthogonal Diagonalization}
\begin{enumerate}
    \item A square matrix $A$ is called orthogonally diagonalizable if there exists an orthogonal matrix such that $P^TAP$ is a diagonal matrix, the matrix $P$ is said to orthogonally diagonalize $A$
    \item A square matrix $A$ is orthogonally diagonalizable $\Leftrightarrow A$ is symmetric, i.e. $A^T = A$
    \item Steps to diagonalize a matrix of $n$ order: \begin{enumerate}
        \item Find all distinct eigenvalues $\lambda_1, ..., \lambda_k$
        \item For each eigenvalue \begin{enumerate}
            \item find a basis $S_\lambda_i$ for the eigenspace $E_\lambda_i$
            \item use te Gram-Schmidt Process to transform $S_\lambda_i$ to an orthonormal basis $T_\lambda_i$
        \end{enumerate}
        \item Let $T = T_\lambda_1 \cup ... \cup T_lambda_k$ 
    \end{enumerate}
    \item If $A$ is symmetric, eigenvectors from different eigenspaces of $A$ are always orthogonal to each other
\end{enumerate}

\section*{Linear Transformation}
\begin{enumerate}
    \item $T: V \rightarrow W$ is called a linear transformation $\Leftrightarrow T(cu + dv) = cT(u) + dT(v)$
    \item Let $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a linear transformation, \\
    \begin{enumerate}
        \item $T(0) = 0$
        \item If $u_1, ..., u_k \in \mathbb{R}^n$ and $c_1, ..., c_k \in \mathbb{R}$, \\
        then $T(c_1u_1 + ... + c_ku_k) = c_1T(u_1) + ... + c_kT(u_k)$
    \end{enumerate}
\end{enumerate}

\section*{Ranges and Kernels}
\begin{enumerate}
    \item The range $R(T)$ is the column space of a standard matrix
    \item The rank of $T$ is $rank(T) = dim(R(T)) = rank(A)$
    \item The kernel $Ker(T)$ is the nullspace of a standard matrix
    \item The nullity of $T$ is $nullity(T) = dim(Ker(T)) = nullity(A)$
    \item Let $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$, \\
    $rank(T) + nullity(T) = n$
\end{enumerate}

\section*{Invertible Matrix}
The following statements are equivalent:
\begin{enumerate}
    \item $A$ is invertible
    \item The linear system $Ax = 0$ has only the trivial solution
    \item The $RREF$ of $A$ is $I$
    \item $A$ can be expressed as a product of elementary matrices
    \item $det(A) \neq 0$
    \item The rows of $A$ form a basis for $\mathbb{R}^n$
    \item The columns of $A$ form a basis for $\mathbb{R}^n$
    \item $rank(A) = n$
    \item $0$ is not an eigenvalue of $A$
\end{enumerate}

\end{document}
